{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80bd1a0",
   "metadata": {},
   "source": [
    "# 预处理数据\n",
    "\n",
    "包括：去除@，url，最小化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ba3e1",
   "metadata": {},
   "source": [
    "## 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134dec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "# Libraries and packages for text (pre-)processing \n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Version info.:\", sys.version_info)\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"numpy version:\", np.__version__)\n",
    "print(\"skearn version:\", sklearn.__version__)\n",
    "print(\"re version:\", re.__version__)\n",
    "print(\"nltk version:\", nltk.__version__)\n",
    "print(\"Operating system:\", os.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b00e8cb",
   "metadata": {},
   "source": [
    "## 对于文本数据进行清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa42a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Software\\Local Things (Coding)\\comp\\2025大学生建模比赛\\代码\\Twitter-Sentiment-Analysis-and-Tweet-Extraction-main\\data\\train.csv')\n",
    "# df.head()\n",
    "pred_df = pd.read_csv(r\"C:\\Software\\Local Things (Coding)\\comp\\2025大学生建模比赛\\数据\\filtered_data.csv\")\n",
    "print(f\"training data shape: {df.shape} \\n testing data shape: {pred_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d550953",
   "metadata": {},
   "source": [
    "### 导入清洗数据函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227cef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_URL(text):\n",
    "    \"\"\"\n",
    "        Remove URLs from a sample string\n",
    "    \"\"\"\n",
    "    return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "def remove_at_user(text):\n",
    "    \"\"\"\n",
    "        Remove @user from a sample string\n",
    "    \"\"\"\n",
    "    return re.sub(r'@[\\w]+', '', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    \"\"\"\n",
    "        Remove the html in sample text\n",
    "    \"\"\"\n",
    "    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n",
    "    return re.sub(html, \"\", text)\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"\n",
    "        Remove non-ASCII characters \n",
    "    \"\"\"\n",
    "    return re.sub(r'[^\\x00-\\x7f]',r'', text) # or ''.join([x for x in text if x in string.printable]) \n",
    "\n",
    "def remove_special_characters(text):\n",
    "    \"\"\"\n",
    "        Remove special special characters, including symbols, emojis, and other graphic characters\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    \"\"\"\n",
    "        Remove the punctuation\n",
    "    \"\"\"\n",
    "#     return re.sub(r'[]!\"$%&\\'()*+,./:;=#@?[\\\\^_`{|}~-]+', \"\", text)\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def other_clean(text):\n",
    "        \"\"\"\n",
    "            Other manual text cleaning techniques\n",
    "        \"\"\"\n",
    "        # Typos, slang and other\n",
    "        sample_typos_slang = {\n",
    "                                \"w/e\": \"whatever\",\n",
    "                                \"usagov\": \"usa government\",\n",
    "                                \"recentlu\": \"recently\",\n",
    "                                \"ph0tos\": \"photos\",\n",
    "                                \"amirite\": \"am i right\",\n",
    "                                \"exp0sed\": \"exposed\",\n",
    "                                \"<3\": \"love\",\n",
    "                                \"luv\": \"love\",\n",
    "                                \"amageddon\": \"armageddon\",\n",
    "                                \"trfc\": \"traffic\",\n",
    "                                \"16yr\": \"16 year\"\n",
    "                                }\n",
    "\n",
    "        # Acronyms\n",
    "        sample_acronyms =  { \n",
    "                            \"mh370\": \"malaysia airlines flight 370\",\n",
    "                            \"okwx\": \"oklahoma city weather\",\n",
    "                            \"arwx\": \"arkansas weather\",    \n",
    "                            \"gawx\": \"georgia weather\",  \n",
    "                            \"scwx\": \"south carolina weather\",  \n",
    "                            \"cawx\": \"california weather\",\n",
    "                            \"tnwx\": \"tennessee weather\",\n",
    "                            \"azwx\": \"arizona weather\",  \n",
    "                            \"alwx\": \"alabama weather\",\n",
    "                            \"usnwsgov\": \"united states national weather service\",\n",
    "                            \"2mw\": \"tomorrow\"\n",
    "                            }\n",
    "\n",
    "        \n",
    "        # Some common abbreviations \n",
    "        sample_abbr = {\n",
    "                        \"$\" : \" dollar \",\n",
    "                        \"€\" : \" euro \",\n",
    "                        \"4ao\" : \"for adults only\",\n",
    "                        \"a.m\" : \"before midday\",\n",
    "                        \"a3\" : \"anytime anywhere anyplace\",\n",
    "                        \"aamof\" : \"as a matter of fact\",\n",
    "                        \"acct\" : \"account\",\n",
    "                        \"adih\" : \"another day in hell\",\n",
    "                        \"afaic\" : \"as far as i am concerned\",\n",
    "                        \"afaict\" : \"as far as i can tell\",\n",
    "                        \"afaik\" : \"as far as i know\",\n",
    "                        \"afair\" : \"as far as i remember\",\n",
    "                        \"afk\" : \"away from keyboard\",\n",
    "                        \"app\" : \"application\",\n",
    "                        \"approx\" : \"approximately\",\n",
    "                        \"apps\" : \"applications\",\n",
    "                        \"asap\" : \"as soon as possible\",\n",
    "                        \"asl\" : \"age, sex, location\",\n",
    "                        \"atk\" : \"at the keyboard\",\n",
    "                        \"ave.\" : \"avenue\",\n",
    "                        \"aymm\" : \"are you my mother\",\n",
    "                        \"ayor\" : \"at your own risk\", \n",
    "                        \"b&b\" : \"bed and breakfast\",\n",
    "                        \"b+b\" : \"bed and breakfast\",\n",
    "                        \"b.c\" : \"before christ\",\n",
    "                        \"b2b\" : \"business to business\",\n",
    "                        \"b2c\" : \"business to customer\",\n",
    "                        \"b4\" : \"before\",\n",
    "                        \"b4n\" : \"bye for now\",\n",
    "                        \"b@u\" : \"back at you\",\n",
    "                        \"bae\" : \"before anyone else\",\n",
    "                        \"bak\" : \"back at keyboard\",\n",
    "                        \"bbbg\" : \"bye bye be good\",\n",
    "                        \"bbc\" : \"british broadcasting corporation\",\n",
    "                        \"bbias\" : \"be back in a second\",\n",
    "                        \"bbl\" : \"be back later\",\n",
    "                        \"bbs\" : \"be back soon\",\n",
    "                        \"be4\" : \"before\",\n",
    "                        \"bfn\" : \"bye for now\",\n",
    "                        \"blvd\" : \"boulevard\",\n",
    "                        \"bout\" : \"about\",\n",
    "                        \"brb\" : \"be right back\",\n",
    "                        \"bros\" : \"brothers\",\n",
    "                        \"brt\" : \"be right there\",\n",
    "                        \"bsaaw\" : \"big smile and a wink\",\n",
    "                        \"btw\" : \"by the way\",\n",
    "                        \"bwl\" : \"bursting with laughter\",\n",
    "                        \"c/o\" : \"care of\",\n",
    "                        \"cet\" : \"central european time\",\n",
    "                        \"cf\" : \"compare\",\n",
    "                        \"cia\" : \"central intelligence agency\",\n",
    "                        \"csl\" : \"can not stop laughing\",\n",
    "                        \"cu\" : \"see you\",\n",
    "                        \"cul8r\" : \"see you later\",\n",
    "                        \"cv\" : \"curriculum vitae\",\n",
    "                        \"cwot\" : \"complete waste of time\",\n",
    "                        \"cya\" : \"see you\",\n",
    "                        \"cyt\" : \"see you tomorrow\",\n",
    "                        \"dae\" : \"does anyone else\",\n",
    "                        \"dbmib\" : \"do not bother me i am busy\",\n",
    "                        \"diy\" : \"do it yourself\",\n",
    "                        \"dm\" : \"direct message\",\n",
    "                        \"dwh\" : \"during work hours\",\n",
    "                        \"e123\" : \"easy as one two three\",\n",
    "                        \"eet\" : \"eastern european time\",\n",
    "                        \"eg\" : \"example\",\n",
    "                        \"embm\" : \"early morning business meeting\",\n",
    "                        \"encl\" : \"enclosed\",\n",
    "                        \"encl.\" : \"enclosed\",\n",
    "                        \"etc\" : \"and so on\",\n",
    "                        \"faq\" : \"frequently asked questions\",\n",
    "                        \"fawc\" : \"for anyone who cares\",\n",
    "                        \"fb\" : \"facebook\",\n",
    "                        \"fc\" : \"fingers crossed\",\n",
    "                        \"fig\" : \"figure\",\n",
    "                        \"fimh\" : \"forever in my heart\", \n",
    "                        \"ft.\" : \"feet\",\n",
    "                        \"ft\" : \"featuring\",\n",
    "                        \"ftl\" : \"for the loss\",\n",
    "                        \"ftw\" : \"for the win\",\n",
    "                        \"fwiw\" : \"for what it is worth\",\n",
    "                        \"fyi\" : \"for your information\",\n",
    "                        \"g9\" : \"genius\",\n",
    "                        \"gahoy\" : \"get a hold of yourself\",\n",
    "                        \"gal\" : \"get a life\",\n",
    "                        \"gcse\" : \"general certificate of secondary education\",\n",
    "                        \"gfn\" : \"gone for now\",\n",
    "                        \"gg\" : \"good game\",\n",
    "                        \"gl\" : \"good luck\",\n",
    "                        \"glhf\" : \"good luck have fun\",\n",
    "                        \"gmt\" : \"greenwich mean time\",\n",
    "                        \"gmta\" : \"great minds think alike\",\n",
    "                        \"gn\" : \"good night\",\n",
    "                        \"g.o.a.t\" : \"greatest of all time\",\n",
    "                        \"goat\" : \"greatest of all time\",\n",
    "                        \"goi\" : \"get over it\",\n",
    "                        \"gps\" : \"global positioning system\",\n",
    "                        \"gr8\" : \"great\",\n",
    "                        \"gratz\" : \"congratulations\",\n",
    "                        \"gyal\" : \"girl\",\n",
    "                        \"h&c\" : \"hot and cold\",\n",
    "                        \"hp\" : \"horsepower\",\n",
    "                        \"hr\" : \"hour\",\n",
    "                        \"hrh\" : \"his royal highness\",\n",
    "                        \"ht\" : \"height\",\n",
    "                        \"ibrb\" : \"i will be right back\",\n",
    "                        \"ic\" : \"i see\",\n",
    "                        \"icq\" : \"i seek you\",\n",
    "                        \"icymi\" : \"in case you missed it\",\n",
    "                        \"idc\" : \"i do not care\",\n",
    "                        \"idgadf\" : \"i do not give a damn fuck\",\n",
    "                        \"idgaf\" : \"i do not give a fuck\",\n",
    "                        \"idk\" : \"i do not know\",\n",
    "                        \"ie\" : \"that is\",\n",
    "                        \"i.e\" : \"that is\",\n",
    "                        \"ifyp\" : \"i feel your pain\",\n",
    "                        \"IG\" : \"instagram\",\n",
    "                        \"iirc\" : \"if i remember correctly\",\n",
    "                        \"ilu\" : \"i love you\",\n",
    "                        \"ily\" : \"i love you\",\n",
    "                        \"imho\" : \"in my humble opinion\",\n",
    "                        \"imo\" : \"in my opinion\",\n",
    "                        \"imu\" : \"i miss you\",\n",
    "                        \"iow\" : \"in other words\",\n",
    "                        \"irl\" : \"in real life\",\n",
    "                        \"j4f\" : \"just for fun\",\n",
    "                        \"jic\" : \"just in case\",\n",
    "                        \"jk\" : \"just kidding\",\n",
    "                        \"jsyk\" : \"just so you know\",\n",
    "                        \"l8r\" : \"later\",\n",
    "                        \"lb\" : \"pound\",\n",
    "                        \"lbs\" : \"pounds\",\n",
    "                        \"ldr\" : \"long distance relationship\",\n",
    "                        \"lmao\" : \"laugh my ass off\",\n",
    "                        \"lmfao\" : \"laugh my fucking ass off\",\n",
    "                        \"lol\" : \"laughing out loud\",\n",
    "                        \"ltd\" : \"limited\",\n",
    "                        \"ltns\" : \"long time no see\",\n",
    "                        \"m8\" : \"mate\",\n",
    "                        \"mf\" : \"motherfucker\",\n",
    "                        \"mfs\" : \"motherfuckers\",\n",
    "                        \"mfw\" : \"my face when\",\n",
    "                        \"mofo\" : \"motherfucker\",\n",
    "                        \"mph\" : \"miles per hour\",\n",
    "                        \"mr\" : \"mister\",\n",
    "                        \"mrw\" : \"my reaction when\",\n",
    "                        \"ms\" : \"miss\",\n",
    "                        \"mte\" : \"my thoughts exactly\",\n",
    "                        \"nagi\" : \"not a good idea\",\n",
    "                        \"nbc\" : \"national broadcasting company\",\n",
    "                        \"nbd\" : \"not big deal\",\n",
    "                        \"nfs\" : \"not for sale\",\n",
    "                        \"ngl\" : \"not going to lie\",\n",
    "                        \"nhs\" : \"national health service\",\n",
    "                        \"nrn\" : \"no reply necessary\",\n",
    "                        \"nsfl\" : \"not safe for life\",\n",
    "                        \"nsfw\" : \"not safe for work\",\n",
    "                        \"nth\" : \"nice to have\",\n",
    "                        \"nvr\" : \"never\",\n",
    "                        \"nyc\" : \"new york city\",\n",
    "                        \"oc\" : \"original content\",\n",
    "                        \"og\" : \"original\",\n",
    "                        \"ohp\" : \"overhead projector\",\n",
    "                        \"oic\" : \"oh i see\",\n",
    "                        \"omdb\" : \"over my dead body\",\n",
    "                        \"omg\" : \"oh my god\",\n",
    "                        \"omw\" : \"on my way\",\n",
    "                        \"p.a\" : \"per annum\",\n",
    "                        \"p.m\" : \"after midday\",\n",
    "                        \"pm\" : \"prime minister\",\n",
    "                        \"poc\" : \"people of color\",\n",
    "                        \"pov\" : \"point of view\",\n",
    "                        \"pp\" : \"pages\",\n",
    "                        \"ppl\" : \"people\",\n",
    "                        \"prw\" : \"parents are watching\",\n",
    "                        \"ps\" : \"postscript\",\n",
    "                        \"pt\" : \"point\",\n",
    "                        \"ptb\" : \"please text back\",\n",
    "                        \"pto\" : \"please turn over\",\n",
    "                        \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "                        \"ratchet\" : \"rude\",\n",
    "                        \"rbtl\" : \"read between the lines\",\n",
    "                        \"rlrt\" : \"real life retweet\", \n",
    "                        \"rofl\" : \"rolling on the floor laughing\",\n",
    "                        \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "                        \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "                        \"rt\" : \"retweet\",\n",
    "                        \"ruok\" : \"are you ok\",\n",
    "                        \"sfw\" : \"safe for work\",\n",
    "                        \"sk8\" : \"skate\",\n",
    "                        \"smh\" : \"shake my head\",\n",
    "                        \"sq\" : \"square\",\n",
    "                        \"srsly\" : \"seriously\", \n",
    "                        \"ssdd\" : \"same stuff different day\",\n",
    "                        \"tbh\" : \"to be honest\",\n",
    "                        \"tbs\" : \"tablespooful\",\n",
    "                        \"tbsp\" : \"tablespooful\",\n",
    "                        \"tfw\" : \"that feeling when\",\n",
    "                        \"thks\" : \"thank you\",\n",
    "                        \"tho\" : \"though\",\n",
    "                        \"thx\" : \"thank you\",\n",
    "                        \"tia\" : \"thanks in advance\",\n",
    "                        \"til\" : \"today i learned\",\n",
    "                        \"tl;dr\" : \"too long i did not read\",\n",
    "                        \"tldr\" : \"too long i did not read\",\n",
    "                        \"tmb\" : \"tweet me back\",\n",
    "                        \"tntl\" : \"trying not to laugh\",\n",
    "                        \"ttyl\" : \"talk to you later\",\n",
    "                        \"u\" : \"you\",\n",
    "                        \"u2\" : \"you too\",\n",
    "                        \"u4e\" : \"yours for ever\",\n",
    "                        \"utc\" : \"coordinated universal time\",\n",
    "                        \"w/\" : \"with\",\n",
    "                        \"w/o\" : \"without\",\n",
    "                        \"w8\" : \"wait\",\n",
    "                        \"wassup\" : \"what is up\",\n",
    "                        \"wb\" : \"welcome back\",\n",
    "                        \"wtf\" : \"what the fuck\",\n",
    "                        \"wtg\" : \"way to go\",\n",
    "                        \"wtpa\" : \"where the party at\",\n",
    "                        \"wuf\" : \"where are you from\",\n",
    "                        \"wuzup\" : \"what is up\",\n",
    "                        \"wywh\" : \"wish you were here\",\n",
    "                        \"yd\" : \"yard\",\n",
    "                        \"ygtr\" : \"you got that right\",\n",
    "                        \"ynk\" : \"you never know\",\n",
    "                        \"zzz\" : \"sleeping bored and tired\"\n",
    "                        }\n",
    "            \n",
    "        sample_typos_slang_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_typos_slang.keys()) + r')(?!\\w)')\n",
    "        sample_acronyms_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_acronyms.keys()) + r')(?!\\w)')\n",
    "        sample_abbr_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_abbr.keys()) + r')(?!\\w)')\n",
    "        \n",
    "        text = sample_typos_slang_pattern.sub(lambda x: sample_typos_slang[x.group()], text)\n",
    "        text = sample_acronyms_pattern.sub(lambda x: sample_acronyms[x.group()], text)\n",
    "        text = sample_abbr_pattern.sub(lambda x: sample_abbr[x.group()], text)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a88da",
   "metadata": {},
   "source": [
    "### 清理文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e05c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from autocorrect import Speller\n",
    "\n",
    "def preprocess(df, cols='text'):\n",
    "    # 处理数据\n",
    "    cols_after = cols + '_after'\n",
    "    df[cols] = df[cols].astype(str)  # 转换为字符串\n",
    "    print(f'Original...{df.shape}')\n",
    "    print('============================')\n",
    "    \n",
    "    print(f'Preprocessing...{df.shape}')\n",
    "    print(f'Lowercase...{df.shape}')\n",
    "    df[cols_after] = df[cols].apply(lambda x: x.lower())  # 全部转为小写\n",
    "    print(f'Contraction...{df.shape}')\n",
    "    df[cols_after] = df[cols_after].apply(lambda x: contractions.fix(x))  # 处理缩写\n",
    "    print(f'Remove URL...{df.shape}')\n",
    "    df[cols_after] = df[cols_after].apply(lambda x: remove_URL(x))  # 去除URL\n",
    "    print(f'Remove HTML...{df.shape}')\n",
    "    df[cols_after] = df[cols_after].apply(lambda x: remove_html(x))  # 去除HTML标签\n",
    "    print(f'Remove @user...{df.shape}')\n",
    "    df[cols_after] = df[cols_after].apply(lambda x: remove_at_user(x))  # 去除@用户\n",
    "    print(f'Remove non-ascii...{df.shape}')\n",
    "    df[cols_after] = df[cols_after].apply(lambda x: remove_non_ascii(x))  # 去除表情符号\n",
    "    print(f'Remove special characters...{df.shape}')\n",
    "    df[cols_after] = df[cols_after].apply(lambda x: remove_special_characters(x))  # 去除特殊字符\n",
    "    print(f'Remove punctuations...{df.shape}')\n",
    "    df[cols_after] = df[cols_after].apply(lambda x: remove_punct(x))  # 去除标点符号\n",
    "    print(f'Other clean...{df.shape}')\n",
    "    df[cols_after] = df[cols_after].apply(lambda x: other_clean(x))  # 其他清理\n",
    "    # print()\n",
    "    # print(f'Auto correct...')\n",
    "    # df[cols_after] = df[cols_after].apply(lambda x: TextBlob(x).correct().string)  # 自动纠错\n",
    "\n",
    "    # 分词和拼写检查\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    spell = Speller(lang=\"en\")\n",
    "    print(f'Tokenization...{df.shape}')\n",
    "    df[f\"{cols}_token\"] = df[cols_after].apply(lambda x: tokenizer.tokenize(x))  # 去除标点符号并分词\n",
    "    print(f'Spell check...{df.shape}')\n",
    "    df[cols_after] = df[cols_after].apply(lambda x: [spell(i) for i in x])  # 拼写检查\n",
    "    print(f'Done...{df.shape}')\n",
    "    # df[cols_after] = df[cols_after].apply(lambda x: re.sub(r'^\\s*|\\s\\s*', ' ', x).strip()) # 空格分词\n",
    "    # print(f'Space tokenization...')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd4af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Software\\Local Things (Coding)\\comp\\2025大学生建模比赛\\代码\\比赛项目代码\\处理后数据集\")\n",
    "\n",
    "df_processed = preprocess(df,cols = 'text')\n",
    "df_processed.to_csv('train_cleaned.csv',index=False)\n",
    "# df_processed.head()\n",
    "pred_df_processed = preprocess(pred_df,cols = 'full_text')\n",
    "pred_df_processed.to_csv('test_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc6b0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f972b9bf",
   "metadata": {},
   "source": [
    "# 词云图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e1e45",
   "metadata": {},
   "source": [
    "## 训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc5d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Software\\Local Things (Coding)\\comp\\2025大学生建模比赛\\代码\\比赛项目代码\\处理后数据集\\train_cleaned.csv\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c9029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 定义一个函数来生成词云图\n",
    "def generate_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "# 提取每个类别中的 tokens 并合并成字符串\n",
    "categories = ['positive', 'negative', 'neutral']\n",
    "texts = {}\n",
    "\n",
    "for category in categories:\n",
    "    category_tokens = []\n",
    "    for tokens in df[df['sentiment'] == category]['text_token']:\n",
    "        category_tokens.extend(eval(tokens))  # 使用 eval() 转换字符串表示的列表\n",
    "    texts[category] = ' '.join(category_tokens)\n",
    "\n",
    "# 创建子图并显示每个类别的词云图\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    plt.subplot(1, 3, i + 1)  # 1 行 3 列的子图布局\n",
    "    generate_wordcloud(texts[category], category.capitalize())\n",
    "\n",
    "plt.tight_layout()  # 调整子图之间的间距\n",
    "plt.savefig('train data wordcloud.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "# 定义一个函数来计算每个token的概率\n",
    "def calculate_token_probabilities(tokens):\n",
    "    token_counts = Counter(tokens)\n",
    "    total_tokens = len(tokens)\n",
    "    token_probabilities = {token: count / total_tokens for token, count in token_counts.items()}\n",
    "    return token_probabilities\n",
    "\n",
    "# 提取每个类别中的 tokens 并合并成列表\n",
    "categories = ['positive', 'negative', 'neutral']\n",
    "category_tokens = {category: [] for category in categories}\n",
    "\n",
    "for category in categories:\n",
    "    for tokens in df[df['sentiment'] == category]['text_token']:\n",
    "        category_tokens[category].extend(eval(tokens))  # 使用 eval() 转换字符串表示的列表\n",
    "\n",
    "# 计算每个类别中每个token的概率\n",
    "token_probabilities = {category: calculate_token_probabilities(tokens) for category, tokens in category_tokens.items()}\n",
    "\n",
    "# 为了绘制概率分布图，我们需要将token和其概率转换为有序的列表\n",
    "sorted_token_probabilities = {category: sorted(probs.items(), key=lambda item: item[1], reverse=True) for category, probs in token_probabilities.items()}\n",
    "\n",
    "# 创建子图并显示每个类别的概率分布图\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    plt.subplot(1, 3, i + 1)  # 1 行 3 列的子图布局\n",
    "    tokens, probabilities = zip(*sorted_token_probabilities[category][:20])  # 只取前20个token\n",
    "    plt.bar(tokens, probabilities)\n",
    "    plt.title(f'{category.capitalize()} Token 概率分布图')\n",
    "    plt.xlabel('Tokens')\n",
    "    plt.ylabel('概率')\n",
    "    plt.xticks(rotation=45)  # 旋转x轴标签以便更好地显示\n",
    "\n",
    "plt.tight_layout()  # 调整子图之间的间距\n",
    "plt.savefig('train_data_token_distribution.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5dd168",
   "metadata": {},
   "source": [
    "## 预测数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4726fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Software\\Local Things (Coding)\\comp\\2025大学生建模比赛\\代码\\比赛项目代码\\处理后数据集\\test_cleaned.csv\")\n",
    "print(df.columns)\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdcf155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 提取 full_text_token 列，并将所有的 tokens 合并成一个大的文本字符串\n",
    "all_tokens = []\n",
    "for tokens in df['full_text_token']:\n",
    "    all_tokens.extend(eval(tokens))  # 使用 eval() 转换字符串表示的列表\n",
    "\n",
    "# 将所有 tokens 合并成一个大字符串\n",
    "text = ' '.join(all_tokens)\n",
    "\n",
    "# 生成词云图\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "# 显示词云图\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # 不显示坐标轴\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载数据\n",
    "data_path = r\"C:\\Software\\Local Things (Coding)\\comp\\2025大学生建模比赛\\代码\\比赛项目代码\\处理后数据集\\test_cleaned.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# 生成整体词云\n",
    "all_tokens = []\n",
    "for tokens in df['full_text_token']:\n",
    "    all_tokens.extend(eval(tokens))\n",
    "    \n",
    "overall_text = ' '.join(all_tokens)\n",
    "overall_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(overall_text)\n",
    "\n",
    "# 准备子图布局\n",
    "unique_names = df['name'].unique()\n",
    "num_total = len(unique_names) + 1  # 增加整体词云\n",
    "cols = 4\n",
    "rows = (num_total // cols) + (1 if num_total % cols != 0 else 0)\n",
    "\n",
    "# 创建包含整体词云的画布\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(15, rows * 2))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 第1个子图绘制整体词云\n",
    "axes[0].imshow(overall_wordcloud, interpolation='bilinear')\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title(\"Overall WordCloud\", fontsize=12)\n",
    "\n",
    "# 遍历每个name绘制子图\n",
    "for idx, (name, group) in enumerate(df.groupby('name'), start=1):  # 从索引1开始\n",
    "    name_tokens = []\n",
    "    for tokens in group['full_text_token']:\n",
    "        name_tokens.extend(eval(tokens))\n",
    "    \n",
    "    text = ' '.join(name_tokens)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    \n",
    "    axes[idx].imshow(wordcloud, interpolation='bilinear')\n",
    "    axes[idx].axis('off')\n",
    "    axes[idx].set_title(f\"{name}\", fontsize=12)\n",
    "\n",
    "# 隐藏空白子图\n",
    "for idx in range(num_total, len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "# 调整布局并保存\n",
    "plt.tight_layout()\n",
    "plt.savefig('词云图.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da42ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 设置头像文件夹路径\n",
    "folder_path = r'C:\\Users\\Fisher Man\\Downloads\\TweetCrawler\\头像列表'  # 替换为你的文件夹路径\n",
    "\n",
    "# 获取文件夹中所有文件的路径\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "# 创建完整的文件路径列表\n",
    "image_paths = [os.path.join(folder_path, file) for file in image_files]\n",
    "\n",
    "# 加载所有头像图片\n",
    "images = [Image.open(img_path) for img_path in image_paths]\n",
    "\n",
    "# 调整图片的尺寸，以便统一拼接\n",
    "image_size = (100, 100)  # 统一尺寸 100x100，可以根据需要调整\n",
    "images_resized = [img.resize(image_size) for img in images]\n",
    "\n",
    "# 拼接设置（比如按每行4个头像）\n",
    "cols = 6\n",
    "rows = (len(images_resized) // cols) + (1 if len(images_resized) % cols != 0 else 0)\n",
    "\n",
    "# 创建一个新图像，用于拼接所有头像\n",
    "# 计算拼接后的图片大小\n",
    "width = cols * image_size[0]\n",
    "height = rows * image_size[1]\n",
    "result_image = Image.new('RGB', (width, height))\n",
    "\n",
    "# 将头像按行列排列\n",
    "for index, img in enumerate(images_resized):\n",
    "    row = index // cols\n",
    "    col = index % cols\n",
    "    # 计算每个头像在大图中的位置\n",
    "    result_image.paste(img, (col * image_size[0], row * image_size[1]))\n",
    "\n",
    "# 保存结果图像\n",
    "result_image.save(r'拼接后的头像图.png')\n",
    "\n",
    "# 展示拼接后的图像\n",
    "result_image.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
