{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对于超级富豪的筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bill_path = r\"C:\\Software\\Local Things (Coding)\\comp\\2025大学生建模比赛\\数据\\all_billionaires_1997_2024.csv\"\n",
    "bill_list = pd.read_csv(bill_path)\n",
    "\n",
    "bill_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Wrangler 生成的单元格。\n",
    "\"\"\"\n",
    "def clean_data(bill_list):\n",
    "    # 删除列: 'first_name' 中缺少数据的行\n",
    "    bill_list = bill_list.dropna(subset=['first_name'])\n",
    "    # 根据列筛选行: 'year'\n",
    "    bill_list = bill_list[bill_list['year'] > 2019]\n",
    "    # 按列排序: 'full_name' (升序)\n",
    "    bill_list = bill_list.sort_values(['full_name'])\n",
    "    # 根据列筛选行: 'rank'\n",
    "    bill_list = bill_list[bill_list['rank'] <= 200]\n",
    "    return bill_list\n",
    "\n",
    "bill_list_clean = clean_data(bill_list.copy())\n",
    "bill_list_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = bill_list_clean['full_name'].value_counts()\n",
    "value_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bill_list_final = pd.read_csv(r'bill_list 2019-2024.csv')\n",
    "bill_list_final = pd.read_excel(r'C:\\Software\\Local Things (Coding)\\comp\\2025大学生建模比赛\\数据\\最终30名富豪名单.xlsx')\n",
    "name_list = bill_list_final['full_name']\n",
    "data = bill_list_clean.loc[bill_list_clean['full_name'].isin(name_list)]\n",
    "# data.head()\n",
    "data.to_csv('bill_list_final.csv', index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理json文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量合并json文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "json_folder = r\"C:\\Users\\Fisher Man\\Downloads\\TweetCrawler\"\n",
    "data = []\n",
    "\n",
    "# 遍历主文件夹下的所有子文件夹\n",
    "for person_folder in os.listdir(json_folder):\n",
    "    person_path = os.path.join(json_folder, person_folder)\n",
    "    if os.path.isdir(person_path):\n",
    "        # 遍历子文件夹中的所有JSON文件\n",
    "        for json_file in os.listdir(person_path):\n",
    "            if json_file.endswith('.json'):\n",
    "                json_file_path = os.path.join(person_path, json_file)\n",
    "                with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                    tweet = json.load(f)\n",
    "                    # 确保提取的是列表中的第一个元素（如果有的话）\n",
    "                    if isinstance(tweet, list):\n",
    "                        tweet = tweet[0]\n",
    "                    # 提取所需的信息\n",
    "                    created_at = tweet.get('createdAt')\n",
    "                    full_text = tweet.get('fullText')\n",
    "                    # 假设推特作者的名字就是子文件夹的名字\n",
    "                    data.append([created_at, person_folder, full_text])\n",
    "\n",
    "# 将提取的数据转换为DataFrame\n",
    "df = pd.DataFrame(data, columns=['time', 'name', 'full_text'])\n",
    "# 最后两列需要删去\n",
    "df.to_csv('data.csv', index=False,encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并数据量大的超级富豪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "giant5_path = r\"C:\\Users\\Fisher Man\\Downloads\\TweetCrawler\\现有数据集\\giant5.xlsx\"\n",
    "small_path = r\"C:\\Software\\Local Things (Coding)\\comp\\2025大学生建模比赛\\数据\\data.csv\"\n",
    "elon_path = r\"C:\\Users\\Fisher Man\\Downloads\\TweetCrawler\\现有数据集\\all_musk_posts.csv\"\n",
    "giant5 = pd.read_excel(giant5_path)\n",
    "small = pd.read_csv(small_path)\n",
    "elon = pd.read_csv(elon_path)\n",
    "# giant5.head()\n",
    "print(giant5.columns)\n",
    "print(small.columns)\n",
    "print(elon.columns)\n",
    "\n",
    "print(f\"giant5 shape: {giant5.shape} \\n small shape: {small.shape} \\n elon shape: {elon.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_indicators = ['time', 'name', 'full_text']\n",
    "giant5.rename(columns={'date': 'time', 'user.username': 'name','rawContent': 'full_text'}, inplace=True)\n",
    "elon.rename(columns = {'createdAt':'time','fullText': 'full_text'},inplace=True)\n",
    "elon['name'] = 'elonmusk'\n",
    "giant5_tbmerge = giant5[key_indicators]\n",
    "elon_tbmerge = elon[key_indicators]\n",
    "# timestamp: giant 5:2025-04-04T18:29:29.000Z, elon:2023-05-07 10:36:27+00:00, small:2019-02-12T15:39:52.000Z\n",
    "print(f'timestamp processing...for giant5')\n",
    "giant5_tbmerge['time'] = pd.to_datetime(giant5_tbmerge['time']).dt.tz_convert('UTC')\n",
    "print(f'timestamp processing...for elon')\n",
    "elon_tbmerge['time'] = pd.to_datetime(elon_tbmerge['time']).dt.tz_convert('UTC')\n",
    "print(f'timestamp processing...for small')\n",
    "# small['time'] = pd.to_datetime(small['time']).dt.tz_convert('UTC',format = 'ISO8601')\n",
    "# small['time'] = pd.to_datetime(small['time'], utc=True)\n",
    "# small['time'] = pd.to_datetime(small['time'], format='%a %b %d %H:%M:%S %z %Y')\n",
    "small['time'] = pd.to_datetime(small['time']).dt.tz_convert('UTC')\n",
    "# print(giant5_tbmerge.head())\n",
    "print(f\"giant5_tbmerge shape: {giant5_tbmerge.shape}\")\n",
    "print(f\"giant5_tbmerge columns: {giant5_tbmerge.columns}\")\n",
    "print(f\"elon_tbmerge shape: {elon_tbmerge.shape}\")\n",
    "print(f\"elon_tbmerge columns: {elon_tbmerge.columns}\")\n",
    "print(f\"small columns: {small.columns}\")\n",
    "print(f\"small shape: {small.shape}\")\n",
    "# print(f\"giant5_tbmerge.dtypes: {giant5_tbmerge.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([giant5_tbmerge, elon_tbmerge, small], axis=0)\n",
    "merged_df.set_index('time', inplace=True)\n",
    "merged_df.sort_index(inplace=True)\n",
    "\n",
    "print(merged_df.shape)\n",
    "\n",
    "merged_df.drop_duplicates(subset='full_text', inplace=True)\n",
    "\n",
    "print(merged_df.shape)\n",
    "\n",
    "merged_df.reset_index(inplace=True)\n",
    "merged_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义目标名字列表\n",
    "target_names = [\n",
    "    \"elonmusk\",\n",
    "    \"BillGates\",\n",
    "    \"JeffBezos\",\n",
    "    \"masason\",\n",
    "    \"ericschmidt\",\n",
    "    \"RicardoBSalinas\",\n",
    "    \"WarrenBuffett\",\n",
    "    \"RayDalio\",\n",
    "    \"udaykotak\",\n",
    "    \"AlikoDangoto\",\n",
    "    \"findkd\",\n",
    "    \"MichaelDell\",\n",
    "    \"JackMa\",\n",
    "    \"rupertmurdoch\",\n",
    "    \"leijun\",\n",
    "    \"carlosslim\",\n",
    "    \"StevenACohen2\",\n",
    "    \"MukeshDhiAmbani\",\n",
    "    \"mackenziescott\",  # 贝索斯前妻\n",
    "    \"larryellison\",\n",
    "    \"mcannonbrookes\",\n",
    "    \"laurenepowell\",\n",
    "    \"Steven_Ballmer\",\n",
    "    \"scottfarkas\",\n",
    "    \"esaverin\",\n",
    "    \"lemannoficial\",  # 85岁，official号\n",
    "    \"_AmancioOrtega\",\n",
    "    \"ShivNadarFDN\",\n",
    "    \"Eyalo365\"\n",
    "]\n",
    "\n",
    "# 打印目标名字列表长度以供检查\n",
    "print(len(target_names))\n",
    "\n",
    "# 时间区间\n",
    "start_time = pd.to_datetime('2019-01-01 00:00:00').tz_localize('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['time'] = pd.to_datetime(merged_df['time'],utc=True)\n",
    "filtered_df = merged_df[(merged_df['time'] > start_time) & (merged_df['name'].isin(target_names))]\n",
    "filtered_df.sort_values(by='time', inplace=True)\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "print(filtered_df.shape)\n",
    "filtered_df.to_csv('filtered_data.csv', index=False)\n",
    "# print(filtered_df['name'].count())\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Wrangler 生成的单元格。\n",
    "该函数用于统计每个用户名的完整文本出现次数。\n",
    "\"\"\"\n",
    "def calculate_user_statistics(user_data):\n",
    "    # 已对列 'name' 执行 1 个聚合 分组\n",
    "    user_statistics = user_data.groupby(['name']).agg(full_text_count=('full_text', 'count')).reset_index()\n",
    "    return user_statistics\n",
    "\n",
    "user_statistics_df = calculate_user_statistics(filtered_df.copy())\n",
    "user_statistics_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Wrangler 生成的单元格。\n",
    "\"\"\"\n",
    "def eyalo_data(filtered_df):\n",
    "    # 根据列筛选行: 'name'\n",
    "    filtered_df = filtered_df[filtered_df['name'] == \"Eyalo365\"]\n",
    "    return filtered_df\n",
    "\n",
    "eyalo = eyalo_data(filtered_df.copy())\n",
    "eyalo.to_csv(\"eyalo.csv\", index=False)\n",
    "eyalo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation for Eyalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "eyalo = pd.read_csv('eyalo.csv')\n",
    "eyalo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eyalo['txt_length'] = eyalo['full_text'].str.len()\n",
    "total_length = eyalo['txt_length'].sum()\n",
    "print(f\"调用api需要的字符数：{total_length/1000000:.2f}百万\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 合并指数文件并计算相应的波动率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "波动率函数\n",
    "\n",
    "Step 1:\n",
    "\n",
    "$$ R_n = ln(\\frac{S_n}{S_{n-1}})$$\n",
    "其中，$S_n$为第n天的收盘价，$S_{n-1}$为第n-1天的收盘价。\n",
    "\n",
    "Step 2:\n",
    "\n",
    "$$ R_{avg} = \\frac{1}{n}\\sum_{i=1}^n R_i $$\n",
    "we use n = 7 here.\n",
    "\n",
    "Step 3:\n",
    "\n",
    "$$ \\sigma_R = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (R_i - R_{avg})^2} $$\n",
    "\n",
    "Step 4:\n",
    "\n",
    "$$ HV_y = \\sigma_R*\\sqrt{7} $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 设置数据目录\n",
    "main_path = r\"C:\\Software\\Local Things (Coding)\\comp\\2025大学生建模比赛\\数据\\USA index\"\n",
    "\n",
    "# 获取目录下所有 .xlsx 文件\n",
    "file_list = [f for f in os.listdir(main_path) if f.endswith('.xlsx')]\n",
    "vol_period = 7  # 计算未来 7 天波动率的周期\n",
    "dfs = []\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(main_path, file)\n",
    "    # 读取 Excel 文件（默认第一个sheet）\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # 转换“交易日期”为 datetime 类型\n",
    "    df[\"交易日期\"] = pd.to_datetime(df[\"交易日期\"])\n",
    "    \n",
    "    # 处理“收盘价”：去除可能的逗号，并转换为 float\n",
    "    df[\"收盘价\"] = df[\"收盘价\"].astype(str).str.replace(\",\", \"\", regex=False).astype(float)\n",
    "    \n",
    "    # 按“交易日期”升序排序\n",
    "    df = df.sort_values(\"交易日期\").reset_index(drop=True)\n",
    "    \n",
    "    # 用于计算未来 7 天波动率的函数，\n",
    "    # 这里按照公式步骤计算：\n",
    "    # Step 1: 计算日对数收益率 R_t = ln(S_t / S_{t-1}) （共 7 个返回值，需要 8 天收盘价）\n",
    "    # Step 2: 计算7个收益率的平均值\n",
    "    # Step 3: 计算样本标准差 sigma_R = sqrt( sum((R_i-R_avg)^2)/(7-1) )\n",
    "    # Step 4: HV_y = sigma_R * sqrt(7)\n",
    "    def calc_hv(idx, price_series):\n",
    "        # 取从当前日期 idx 开始，连续8天（即当前日及未来7天）的收盘价\n",
    "        subset = price_series.iloc[idx: idx+vol_period+1]\n",
    "        if len(subset) < vol_period+1:\n",
    "            return np.nan\n",
    "        # 计算日对数收益率，共有7个\n",
    "        returns = np.log(subset.values[1:] / subset.values[:-1])\n",
    "        # Step 2: 计算平均收益率\n",
    "        avg_return = returns.mean()\n",
    "        # Step 3: 计算样本标准差（自由度为7-1=6）\n",
    "        sigma_R = np.sqrt(np.sum((returns - avg_return)**2) / (vol_period - 1))\n",
    "        # Step 4: 波动率调整\n",
    "        HV_y = sigma_R * np.sqrt(vol_period)\n",
    "        return HV_y\n",
    "    \n",
    "    # 对每个交易日计算未来7天的波动率（计算基于当日到第7天共8天数据）\n",
    "    df[\"未来7天波动率\"] = [calc_hv(i, df[\"收盘价\"]) for i in range(len(df))]\n",
    "    \n",
    "    # 提取文件名中前缀作为指数名称，例如 \"DJI\" 来自 \"DJI.GI-行情统计-20250408.xlsx\"\n",
    "    index_name = file.split('.')[0]\n",
    "    df[\"指数\"] = index_name\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "# 合并所有指数的数据\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "final_df = final_df.sort_values([\"指数\", \"交易日期\"]).reset_index(drop=True)\n",
    "final_df.to_csv(r\"stock_data.csv\", index=False,encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设 final_df 已经加载并准备好\n",
    "df_panel = final_df[['交易日期', '指数', '未来7天波动率']].copy()\n",
    "\n",
    "# 将 '交易日期' 列转换为时区感知的 datetime 类型\n",
    "df_panel['交易日期'] = pd.to_datetime(df_panel['交易日期']).dt.tz_localize('UTC')\n",
    "\n",
    "# 透视数据以创建宽表\n",
    "df_wide = df_panel.pivot(index='交易日期', columns='指数', values='未来7天波动率')\n",
    "\n",
    "# 打印宽表的列名\n",
    "print(df_wide.columns)\n",
    "\n",
    "# 设置开始时间\n",
    "start_time = pd.Timestamp('2019-01-01 00:00:00', tz='UTC')\n",
    "\n",
    "# 过滤数据，保留交易日期大于等于开始时间的记录\n",
    "df_wide = df_wide[df_wide.index >= start_time]\n",
    "\n",
    "# 将结果保存为CSV文件\n",
    "df_wide.to_csv('stock_volatility.csv', index=True,encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# 原始数据解析\n",
    "log_data = \"\"\"\n",
    "[您的完整日志数据粘贴在此]\n",
    "\"\"\"\n",
    "\n",
    "# 数据提取模式\n",
    "epoch_pattern = r\"Epoch (\\d+)/(\\d+).*train_acc=([\\d.]+), val_acc=([\\d.]+)\"\n",
    "batches = re.findall(epoch_pattern, log_data)\n",
    "\n",
    "# 数据结构化处理\n",
    "training_records = []\n",
    "for epoch, total_epochs, train_acc, val_acc in batches:\n",
    "    training_records.append({\n",
    "        \"epoch\": int(epoch),\n",
    "        \"total_epochs\": int(total_epochs),\n",
    "        \"train_acc\": float(train_acc),\n",
    "        \"val_acc\": float(val_acc)\n",
    "    })\n",
    "\n",
    "# 关键阶段划分\n",
    "phases = {\n",
    "    \"activation\": slice(0, 25),       # 前5次5 epoch实验\n",
    "    \"optimizer\": slice(25, 45),       # 后续4次5 epoch实验\n",
    "    \"epoch_tuning\": slice(45, 105),   # 不同epoch数实验\n",
    "    \"batch_tuning\": slice(105, 165),  # 批量大小实验\n",
    "    \"final_tuning\": slice(165, None)  # 最终调整\n",
    "}\n",
    "\n",
    "# 可视化设置\n",
    "plt.figure(figsize=(15, 18))\n",
    "plt.suptitle(\"Model Training Analysis\", y=1.02)\n",
    "\n",
    "# 子图1: 验证准确率发展趋势\n",
    "ax1 = plt.subplot(311)\n",
    "val_accs = [x[\"val_acc\"] for x in training_records]\n",
    "ax1.plot(val_accs, 'b-', alpha=0.6, label=\"Validation Accuracy\")\n",
    "ax1.scatter(np.argmax(val_accs), max(val_accs), c='r', \n",
    "          label=f\"Best val_acc: {max(val_accs):.4f}\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.set_xlabel(\"Training Progress\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# 子图2: 阶段验证准确率分布\n",
    "ax2 = plt.subplot(312)\n",
    "phase_labels = [\"Activation\", \"Optimizer\", \"Epoch\", \"Batch\", \"Final\"]\n",
    "phase_accs = [\n",
    "    np.mean([x[\"val_acc\"] for x in training_records[phases[\"activation\"]]]),\n",
    "    np.mean([x[\"val_acc\"] for x in training_records[phases[\"optimizer\"]]]),\n",
    "    np.mean([x[\"val_acc\"] for x in training_records[phases[\"epoch_tuning\"]]]),\n",
    "    np.mean([x[\"val_acc\"] for x in training_records[phases[\"batch_tuning\"]]]),\n",
    "    np.mean([x[\"val_acc\"] for x in training_records[phases[\"final_tuning\"]]])\n",
    "]\n",
    "ax2.bar(phase_labels, phase_accs, alpha=0.6, \n",
    "       color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
    "ax2.set_ylabel(\"Average Val Accuracy\")\n",
    "ax2.set_title(\"Phase Performance Comparison\")\n",
    "ax2.grid(True)\n",
    "\n",
    "# 子图3: 训练/验证曲线\n",
    "ax3 = plt.subplot(313)\n",
    "train_accs = [x[\"train_acc\"] for x in training_records]\n",
    "ax3.plot(train_accs, 'g--', label=\"Training Accuracy\")\n",
    "ax3.plot(val_accs, 'b-', label=\"Validation Accuracy\")\n",
    "ax3.set_ylabel(\"Accuracy\")\n",
    "ax3.set_xlabel(\"Epochs\")\n",
    "ax3.legend()\n",
    "ax3.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 输出最终测试结果\n",
    "test_acc = 0.6705  # 从日志中提取的测试准确率\n",
    "print(f\"Final Test Accuracy: {test_acc:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
