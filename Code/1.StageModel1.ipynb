{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3404c822",
   "metadata": {},
   "source": [
    "# 自然语言处理部分\n",
    "\n",
    "利用自然语言处理为sentiment score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72c1376",
   "metadata": {},
   "source": [
    "## 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8f51ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import torchtext\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614a154",
   "metadata": {},
   "source": [
    "## Read Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221385eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(r\"C:\\Software\\Local Things (Coding)\\comp\\2025大学生建模比赛\\代码\\比赛项目代码\\处理后数据集\\train_cleaned.csv\")\n",
    "test_data = pd.read_csv(r\"C:\\Software\\Local Things (Coding)\\comp\\2025大学生建模比赛\\代码\\比赛项目代码\\处理后数据集\\test_cleaned.csv\")\n",
    "# print (train_data.shape)\n",
    "# print (test_data.shape)\n",
    "print(f\"train_data.columns: {train_data.columns} \\n train_data.shape: {train_data.shape}\")\n",
    "print(f\"test_data.columns: {test_data.columns} \\n test_data.shape: {test_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc664d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data['text_after'][:5])\n",
    "print(train_data['text_token'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a946d07",
   "metadata": {},
   "source": [
    "## Split and Tozkenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 1. 数据预处理\n",
    "# ===============================\n",
    "# 假设 train_data 是已加载的 DataFrame，其中包含 'text_token'、'text' 和 'sentiment' 字段\n",
    "df = train_data.copy()\n",
    "\n",
    "# 先将 sentiment 转为 category 并获取类别数\n",
    "df.sentiment = df.sentiment.astype('category')\n",
    "num_classes = len(df.sentiment.cat.categories)\n",
    "\n",
    "# 保证文本是字符串，且对情感标签编码\n",
    "df.text_token = df.text_token.map(lambda x: str(x))\n",
    "print(f\"before encoding: {df['sentiment'][:5]}\")\n",
    "df.sentiment = df.sentiment.cat.codes  # 此时 sentiment 为整数索引\n",
    "print(f\"after encoding: {df['sentiment'][:5]}\")\n",
    "print(\"划分训练、验证、测试集 ...\")\n",
    "# 划分数据集；这里使用 stratify 保证标签分布均衡\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df.text_token.values,\n",
    "    df.sentiment.values,\n",
    "    stratify=df.sentiment.values,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 从 x_test 中划分验证集（例如前4122个样本）和测试集\n",
    "x_val = x_test[:4122]\n",
    "y_val = y_test[:4122]\n",
    "x_test = x_test[4122:]\n",
    "y_test = y_test[4122:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c4d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 2. 文本分词、构建词汇表及序列填充（使用 torchtext）\n",
    "# ===============================\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# 定义分词器\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "df.text = df.text.astype(str)\n",
    "# 构建词汇表：这里使用 df.text 字段来生成词汇表，并限制最大词汇量为 5000\n",
    "vocab = build_vocab_from_iterator(\n",
    "    (tokenizer(text) for text in df.text.values),\n",
    "    specials=[\"<unk>\"],\n",
    "    # max_tokens=5000\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# 得到 word 到索引的映射字典\n",
    "word_index = vocab.get_stoi()\n",
    "vocab_size = len(vocab)\n",
    "maxlen = 100  # 序列最大长度\n",
    "\n",
    "# 定义将文本转换为整数索引序列的函数\n",
    "def text_to_sequence(text, tokenizer, vocab):\n",
    "    return vocab(tokenizer(text))\n",
    "\n",
    "# 将训练、验证、测试文本转换为序列\n",
    "X_train = [text_to_sequence(text, tokenizer, vocab) for text in x_train]\n",
    "X_val = [text_to_sequence(text, tokenizer, vocab) for text in x_val]\n",
    "X_test = [text_to_sequence(text, tokenizer, vocab) for text in x_test]\n",
    "\n",
    "# 定义填充函数，使每个序列固定为 maxlen\n",
    "def pad_sequence(seq, maxlen=maxlen, padding_value=0):\n",
    "    if len(seq) > maxlen:\n",
    "        return seq[:maxlen]\n",
    "    else:\n",
    "        return seq + [padding_value] * (maxlen - len(seq))\n",
    "\n",
    "X_train = [pad_sequence(seq, maxlen) for seq in X_train]\n",
    "X_val   = [pad_sequence(seq, maxlen) for seq in X_val]\n",
    "X_test  = [pad_sequence(seq, maxlen) for seq in X_test]\n",
    "\n",
    "# 将列表转换为 torch.Tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "X_val   = torch.tensor(X_val, dtype=torch.long)\n",
    "X_test  = torch.tensor(X_test, dtype=torch.long)\n",
    "\n",
    "# 注意：损失函数使用 CrossEntropyLoss 要求标签为整数索引，不做 one_hot 转换\n",
    "y_train = torch.tensor(np.array(y_train, dtype=np.int64).flatten(), dtype=torch.long)\n",
    "y_val   = torch.tensor(np.array(y_val, dtype=np.int64).flatten(), dtype=torch.long)\n",
    "y_test  = torch.tensor(np.array(y_test, dtype=np.int64).flatten(), dtype=torch.long)\n",
    "\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c15175",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed87eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 3. 加载预训练GloVe词向量并构造 embedding_matrix\n",
    "# ===============================\n",
    "pretrained_model = r\"C:\\Dataset\\预训练模型\\glove.twitter.27B.200d.txt\"\n",
    "embeddings_index = {}\n",
    "with open(pretrained_model, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# glove.twitter.27B.200d 的每个词向量维度为 200\n",
    "embedding_dim = 200\n",
    "# 构造 embedding_matrix，行数为 vocab_size，列数为 embedding_dim\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, idx in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "# 转换为 torch.Tensor\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f07933",
   "metadata": {},
   "source": [
    "# Model 1: MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898cd6ab",
   "metadata": {},
   "source": [
    "## MLP Model Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 4. 定义基于预训练Embedding的 PyTorch 模型\n",
    "# ===============================\n",
    "class MLPGlove(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, maxlen, activation_func=\"relu\"):\n",
    "        super(MLPGlove, self).__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 加载预训练词向量，若希望允许 fine-tuning 则保持 requires_grad=True，否则可冻结：\n",
    "        self.embedding.weight.data.copy_(embedding_matrix)\n",
    "        # 若希望冻结预训练层，将下行取消注释：\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "\n",
    "        # 激活函数选择\n",
    "        if activation_func == \"relu\":\n",
    "            self.act = F.relu\n",
    "        elif activation_func == \"elu\":\n",
    "            self.act = F.elu\n",
    "        elif activation_func == \"sigmoid\":\n",
    "            self.act = torch.sigmoid\n",
    "        elif activation_func == \"tanh\":\n",
    "            self.act = torch.tanh\n",
    "        else:\n",
    "            self.act = F.relu\n",
    "\n",
    "        # 模型结构：先全局最大池化，再多层全连接\n",
    "        self.fc1 = nn.Linear(embedding_dim, 30)\n",
    "        self.bn1 = nn.BatchNorm1d(30)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc2 = nn.Linear(30, 10)\n",
    "        self.bn2 = nn.BatchNorm1d(10)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc3 = nn.Linear(10, num_classes)  # 输出类别数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, maxlen)\n",
    "        x = self.embedding(x)  # -> (batch_size, maxlen, embedding_dim)\n",
    "        # Global max pooling：在 maxlen 维度上取最大值，结果形状为 (batch_size, embedding_dim)\n",
    "        x, _ = torch.max(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        # 对输出使用 softmax 得到概率分布（训练时 CrossEntropyLoss 内部已包含 softmax，因此此处可选择不加 softmax）\n",
    "        return F.softmax(x, dim=1)  # -> (batch_size, num_classes)\n",
    "        # 若希望在推断时输出概率，则可以返回 F.softmax(x, dim=1)\n",
    "\n",
    "# ===============================\n",
    "# 5. 定义训练函数（含超参数搜索接口）\n",
    "# ===============================\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def mlp_glove_train(activation, optimizer_name, epochs, batch_size):\n",
    "    # 创建模型实例\n",
    "    model = MLPGlove(vocab_size, embedding_dim, embedding_matrix, maxlen, activation_func=activation)\n",
    "    \n",
    "    # 选择优化器\n",
    "    if optimizer_name.lower() == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "    elif optimizer_name.lower() == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    elif optimizer_name.lower() == \"rmsprop\":\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    # 损失函数：使用 CrossEntropyLoss（目标标签为整数索引）\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 构建 DataLoader\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    history = {\"train_acc\": [], \"val_acc\": []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "        for batch_X, batch_y in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)  # 输出 shape: (batch_size, num_classes)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        correct_val, total_val = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{epochs}\"):\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct_val += (preds == batch_y).sum().item()\n",
    "                total_val += batch_y.size(0)\n",
    "        val_acc = correct_val / total_val\n",
    "        \n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        # 可打印每轮结果以监控训练进程\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "    \n",
    "    return history, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae2454",
   "metadata": {},
   "source": [
    "## MLP hepyerparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd5bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 6. 超参数调优实验与画图\n",
    "# ===============================\n",
    "# 定义待选超参数\n",
    "activation_list = [\"relu\", \"elu\", \"sigmoid\", \"tanh\"]\n",
    "optimizer_list  = [\"adam\", \"SGD\", \"RMSprop\"]\n",
    "epochs_list     = [5, 10, 15, 20]\n",
    "batchsize_list  = [8, 16, 32, 64, 128]\n",
    "\n",
    "# 6.1 选择最佳激活函数（固定 optimizer=adam, epochs=5, batch_size=16）\n",
    "sel_activation = {}\n",
    "for act in activation_list:\n",
    "    history, model = mlp_glove_train(act, \"adam\", 5, 16)\n",
    "    sel_activation[act] = history[\"val_acc\"][-1]\n",
    "    del model\n",
    "best_activation = max(sel_activation, key=sel_activation.get)\n",
    "print(\"best activation function is \", best_activation)\n",
    "\n",
    "# 6.2 选择最佳优化器（固定 activation=best_activation, epochs=5, batch_size=16）\n",
    "sel_optimizer = {}\n",
    "for opt in optimizer_list:\n",
    "    history, model = mlp_glove_train(best_activation, opt, 5, 16)\n",
    "    sel_optimizer[opt] = history[\"val_acc\"][-1]\n",
    "    del model\n",
    "best_optimizer = max(sel_optimizer, key=sel_optimizer.get)\n",
    "print(\"best optimizer is \", best_optimizer)\n",
    "\n",
    "# 6.3 分析不同 epochs 对准确率的影响（固定 activation=best_activation, optimizer=best_optimizer, batch_size=16）\n",
    "acc_train_epoch = {}\n",
    "acc_val_epoch = {}\n",
    "for ep in epochs_list:\n",
    "    history, model = mlp_glove_train(best_activation, best_optimizer, ep, 16)\n",
    "    acc_train_epoch[ep] = history[\"train_acc\"][-1]\n",
    "    acc_val_epoch[ep]   = history[\"val_acc\"][-1]\n",
    "    del model\n",
    "best_epoch = max(acc_val_epoch, key=acc_val_epoch.get)\n",
    "print(\"best epoch is \", best_epoch)\n",
    "\n",
    "df_epoch_train = pd.DataFrame(list(acc_train_epoch.items()), columns=['Epochs', 'Accuracy'])\n",
    "df_epoch_val   = pd.DataFrame(list(acc_val_epoch.items()), columns=['Epochs', 'Accuracy'])\n",
    "df_epoch_train['Epochs'] = df_epoch_train['Epochs'].astype(str)\n",
    "df_epoch_val['Epochs'] = df_epoch_val['Epochs'].astype(str)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_epoch_train['Epochs'], df_epoch_train['Accuracy'], \"r--o\", label=\"train\")\n",
    "plt.plot(df_epoch_val['Epochs'], df_epoch_val['Accuracy'], \"b--o\", label=\"val\")\n",
    "plt.title(\"Accuracy vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "plt.savefig(os.path.join(\"images\", \"acc-epoch-glove-case_identifier.png\"), bbox_inches='tight', dpi=200)\n",
    "\n",
    "# 6.4 分析不同 batch_size 对准确率的影响\n",
    "acc_train_batch = {}\n",
    "acc_val_batch = {}\n",
    "for bs in batchsize_list:\n",
    "    history, model = mlp_glove_train(best_activation, best_optimizer, best_epoch, bs)\n",
    "    acc_train_batch[bs] = history[\"train_acc\"][-1]\n",
    "    acc_val_batch[bs]   = history[\"val_acc\"][-1]\n",
    "    del model\n",
    "best_batch = max(acc_val_batch, key=acc_val_batch.get)\n",
    "print(\"best batchsize is \", best_batch)\n",
    "\n",
    "df_batch_train = pd.DataFrame(list(acc_train_batch.items()), columns=['Batchsize', 'Accuracy'])\n",
    "df_batch_val   = pd.DataFrame(list(acc_val_batch.items()), columns=['Batchsize', 'Accuracy'])\n",
    "df_batch_train['Batchsize'] = df_batch_train['Batchsize'].astype(str)\n",
    "df_batch_val['Batchsize'] = df_batch_val['Batchsize'].astype(str)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_batch_train['Batchsize'], df_batch_train['Accuracy'], \"r--o\", label=\"train\")\n",
    "plt.plot(df_batch_val['Batchsize'], df_batch_val['Accuracy'], \"b--o\", label=\"val\")\n",
    "plt.title(\"Accuracy vs Batchsize\")\n",
    "plt.xlabel(\"Batchsize\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(\"images\", \"acc-batch-glove-case_identifier.png\"), bbox_inches='tight', dpi=200)\n",
    "\n",
    "# ===============================\n",
    "# 7. 最终模型在测试集上的评估\n",
    "# ===============================\n",
    "t0 = time()\n",
    "history, best_model = mlp_glove_train(best_activation, best_optimizer, best_epoch, best_batch)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_batch, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "correct_test, total_test = 0, 0\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = best_model(batch_X)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct_test += (preds == batch_y).sum().item()\n",
    "        total_test += batch_y.size(0)\n",
    "test_accuracy = correct_test / total_test\n",
    "print(\"test accuracy score = \", test_accuracy)\n",
    "t1 = time()\n",
    "print(\"time taken is \", t1 - t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c0299",
   "metadata": {},
   "source": [
    "## Save & Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26781b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存最佳模型参数\n",
    "os.makedirs('models', exist_ok=True)\n",
    "torch.save(best_model.state_dict(), r'models\\MLP_best_model.pth')\n",
    "# activation:sigmoid\n",
    "# optimizer: RMSprop\n",
    "# best_epoch: 10\n",
    "# batch_size: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f31071",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_activation = 'sigmoid'\n",
    "best_optimizer = 'RMSprop'\n",
    "best_epoch = 10\n",
    "best_batch = 8\n",
    "# 加载模型参数到一个新的模型实例\n",
    "new_model = MLPGlove(activation_func=best_activation,vocab_size=vocab_size,embedding_dim=embedding_dim,embedding_matrix=embedding_matrix,maxlen=maxlen)  # 假设 MLP_Glove 是您的模型类\n",
    "new_model.load_state_dict(torch.load(r'models\\MLP_best_model.pth'))\n",
    "new_model.to(device)\n",
    "new_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec073e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['full_text_token'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f239fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备预测数据集（确保转换为数值张量）\n",
    "X_tbp = [text_to_sequence(text,tokenizer,vocab) for text in test_data['full_text_token']]\n",
    "\n",
    "X_tbp = [pad_sequence(seq, maxlen) for seq in X_tbp]\n",
    "text_sequences = torch.tensor(X_tbp, dtype=torch.long)\n",
    "\n",
    "pred_dataset = TensorDataset(text_sequences)\n",
    "pred_loader = DataLoader(pred_dataset, batch_size=best_batch, shuffle=False)\n",
    "\n",
    "# 存储预测结果和概率\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in pred_loader:\n",
    "        batch_X = batch[0].to(device)\n",
    "        outputs = new_model(batch_X)\n",
    "        probs = torch.softmax(outputs, dim=1)  # 转换为概率\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        \n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "# 将结果添加回原始数据\n",
    "test_data['prediction'] = all_preds\n",
    "test_data['probabilities'] = list(np.array(all_probs))  # 存储为概率数组列\n",
    "\n",
    "# 可选：展开概率到独立列\n",
    "prob_df = pd.DataFrame(all_probs, columns=[f'prob_class_{i}' for i in range(len(all_probs[0]))])\n",
    "test_data = pd.concat([test_data, prob_df], axis=1)\n",
    "\n",
    "# 查看结果\n",
    "print(test_data[['full_text', 'prediction', 'probabilities']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4809bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('MLP_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be764ab",
   "metadata": {},
   "source": [
    "# Model 2: CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66252fd4",
   "metadata": {},
   "source": [
    "## CNN Model Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd562624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNGlove(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, maxlen, num_classes, activation_func=\"relu\"):\n",
    "        super(CNNGlove, self).__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 加载预训练词向量\n",
    "        self.embedding.weight.data.copy_(embedding_matrix)\n",
    "        # 若希望冻结预训练层，将下行取消注释：\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "\n",
    "        # 激活函数选择\n",
    "        if activation_func == \"relu\":\n",
    "            self.act = F.relu\n",
    "        elif activation_func == \"elu\":\n",
    "            self.act = F.elu\n",
    "        elif activation_func == \"sigmoid\":\n",
    "            self.act = torch.sigmoid\n",
    "        elif activation_func == \"tanh\":\n",
    "            self.act = torch.tanh\n",
    "        else:\n",
    "            self.act = F.relu\n",
    "\n",
    "        # 模型结构：卷积层 + 最大池化层 + Dropout\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=32, kernel_size=2)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=2)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=2)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        self.conv4 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=2)\n",
    "        self.pool4 = nn.MaxPool1d(2)\n",
    "\n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(16 * ((maxlen - 8) // 16), 64)  # 计算公式：((maxlen - 8) // 16) 为卷积和池化后的长度\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "        # Dropout层\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, maxlen)\n",
    "        x = self.embedding(x)  # -> (batch_size, maxlen, embedding_dim)\n",
    "        x = x.transpose(1, 2)  # -> (batch_size, embedding_dim, maxlen)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)  # -> (batch_size, num_classes)\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cnn_glove_train(activation, optimizer_name, epochs, batch_size):\n",
    "    # 创建模型实例\n",
    "    model = CNNGlove(vocab_size, embedding_dim, embedding_matrix, maxlen, num_classes, activation_func=activation)\n",
    "\n",
    "    # 选择优化器\n",
    "    if optimizer_name.lower() == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "    elif optimizer_name.lower() == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    elif optimizer_name.lower() == \"rmsprop\":\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    # 损失函数：使用 CrossEntropyLoss（目标标签为整数索引）\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 构建 DataLoader\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    history = {\"train_acc\": [], \"val_acc\": []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "        for batch_X, batch_y in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)  # 输出 shape: (batch_size, num_classes)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        correct_val, total_val = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{epochs}\"):\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                correct_val += (preds == batch_y).sum().item()\n",
    "                total_val += batch_y.size(0)\n",
    "        val_acc = correct_val / total_val\n",
    "        \n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        # 可打印每轮结果以监控训练进程\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "    \n",
    "    return history, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a855d59",
   "metadata": {},
   "source": [
    "## CNN hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33384a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from time import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===============================\n",
    "# 6. 超参数调优实验与画图\n",
    "# ===============================\n",
    "# 定义待选超参数\n",
    "activation_list = [\"relu\", \"elu\", \"sigmoid\", \"tanh\"]\n",
    "optimizer_list  = [\"adam\", \"SGD\", \"RMSprop\"]\n",
    "epochs_list     = [5, 10, 15, 20]\n",
    "batchsize_list  = [8, 16, 32, 64, 128]\n",
    "\n",
    "# 6.1 选择最佳激活函数（固定 optimizer=adam, epochs=5, batch_size=16）\n",
    "sel_activation = {}\n",
    "for act in activation_list:\n",
    "    # history, model = cnn_glove_train(, epochs=5, batch_size=16)\n",
    "    history, model = cnn_glove_train(act, \"adam\", 5, 16,)\n",
    "    sel_activation[act] = history[\"val_acc\"][-1]\n",
    "    del model\n",
    "best_activation = max(sel_activation, key=sel_activation.get)\n",
    "print(\"best activation function is \", best_activation)\n",
    "\n",
    "# 6.2 选择最佳优化器（固定 activation=best_activation, epochs=5, batch_size=16）\n",
    "sel_optimizer = {}\n",
    "for opt in optimizer_list:\n",
    "    # history, model = cnn_glove_train(model=CNNGlove,optimizer_name=best_activation, opt, 5, 16)\n",
    "    history, model = cnn_glove_train(best_activation, opt, 5, 16)\n",
    "    sel_optimizer[opt] = history[\"val_acc\"][-1]\n",
    "    del model\n",
    "best_optimizer = max(sel_optimizer, key=sel_optimizer.get)\n",
    "print(\"best optimizer is \", best_optimizer)\n",
    "\n",
    "# 6.3 分析不同 epochs 对准确率的影响（固定 activation=best_activation, optimizer=best_optimizer, batch_size=16）\n",
    "acc_train_epoch = {}\n",
    "acc_val_epoch = {}\n",
    "for ep in epochs_list:\n",
    "    history, model = cnn_glove_train(best_activation, best_optimizer, ep, 16)\n",
    "    acc_train_epoch[ep] = history[\"train_acc\"][-1]\n",
    "    acc_val_epoch[ep]   = history[\"val_acc\"][-1]\n",
    "    del model\n",
    "best_epoch = max(acc_val_epoch, key=acc_val_epoch.get)\n",
    "print(\"best epoch is \", best_epoch)\n",
    "\n",
    "df_epoch_train = pd.DataFrame(list(acc_train_epoch.items()), columns=['Epochs', 'Accuracy'])\n",
    "df_epoch_val   = pd.DataFrame(list(acc_val_epoch.items()), columns=['Epochs', 'Accuracy'])\n",
    "df_epoch_train['Epochs'] = df_epoch_train['Epochs'].astype(str)\n",
    "df_epoch_val['Epochs'] = df_epoch_val['Epochs'].astype(str)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_epoch_train['Epochs'], df_epoch_train['Accuracy'], \"r--o\", label=\"train\")\n",
    "plt.plot(df_epoch_val['Epochs'], df_epoch_val['Accuracy'], \"b--o\", label=\"val\")\n",
    "plt.title(\"Accuracy vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "plt.savefig(os.path.join(\"images\", \"acc-epoch-glove-case_identifier.png\"), bbox_inches='tight', dpi=200)\n",
    "\n",
    "# 6.4 分析不同 batch_size 对准确率的影响\n",
    "acc_train_batch = {}\n",
    "acc_val_batch = {}\n",
    "for bs in batchsize_list:\n",
    "    history, model = cnn_glove_train(best_activation, best_optimizer, best_epoch, bs)\n",
    "    acc_train_batch[bs] = history[\"train_acc\"][-1]\n",
    "    acc_val_batch[bs]   = history[\"val_acc\"][-1]\n",
    "    del model\n",
    "best_batch = max(acc_val_batch, key=acc_val_batch.get)\n",
    "print(\"best batchsize is \", best_batch)\n",
    "\n",
    "df_batch_train = pd.DataFrame(list(acc_train_batch.items()), columns=['Batchsize', 'Accuracy'])\n",
    "df_batch_val   = pd.DataFrame(list(acc_val_batch.items()), columns=['Batchsize', 'Accuracy'])\n",
    "df_batch_train['Batchsize'] = df_batch_train['Batchsize'].astype(str)\n",
    "df_batch_val['Batchsize'] = df_batch_val['Batchsize'].astype(str)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_batch_train['Batchsize'], df_batch_train['Accuracy'], \"r--o\", label=\"train\")\n",
    "plt.plot(df_batch_val['Batchsize'], df_batch_val['Accuracy'], \"b--o\", label=\"val\")\n",
    "plt.title(\"Accuracy vs Batchsize\")\n",
    "plt.xlabel(\"Batchsize\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(\"images\", \"acc-batch-glove-case_identifier.png\"), bbox_inches='tight', dpi=200)\n",
    "\n",
    "# ===============================\n",
    "# 7. 最终模型在测试集上的评估\n",
    "# ===============================\n",
    "t0 = time()\n",
    "history, best_model = cnn_glove_train(best_activation, best_optimizer, best_epoch, best_batch)\n",
    "\n",
    "# 创建 DataLoader 用于测试集\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.long), torch.tensor(y_test, dtype=torch.long))\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_batch, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "\n",
    "correct_test, total_test = 0, 0\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = best_model(batch_X)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct_test += (preds == batch_y).sum().item()\n",
    "        total_test += batch_y.size(0)\n",
    "\n",
    "test_accuracy = correct_test / total_test\n",
    "print(\"test accuracy score = \", test_accuracy)\n",
    "\n",
    "t1 = time()\n",
    "print(\"time taken is \", t1 - t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a53dc6",
   "metadata": {},
   "source": [
    "## Save & Run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631664b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存最佳模型参数\n",
    "os.makedirs('models', exist_ok=True)\n",
    "torch.save(best_model.state_dict(), r'models\\CNN_best_model.pth')\n",
    "# activation:elu\n",
    "# optimizer: adam\n",
    "# best_epoch: 5\n",
    "# batch_size: 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73231623",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_activation = 'elu'\n",
    "best_optimizer = 'adam'\n",
    "best_epoch = 5\n",
    "best_batch = 128\n",
    "# 加载模型参数到一个新的模型实例\n",
    "new_model = CNNGlove(activation_func=best_activation,vocab_size=vocab_size,embedding_dim=embedding_dim,embedding_matrix=embedding_matrix,maxlen=maxlen,num_classes=3)  # 假设 CNN_Glove 是您的模型类\n",
    "new_model.load_state_dict(torch.load(r'models\\CNN_best_model.pth'))\n",
    "new_model.to(device)\n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6eac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_tbp = [text_to_sequence(text,tokenizer,vocab) for text in test_data['full_text_token']]\n",
    "\n",
    "X_tbp = [pad_sequence(seq, maxlen) for seq in X_tbp]\n",
    "text_sequences = torch.tensor(X_tbp, dtype=torch.long)\n",
    "\n",
    "pred_dataset = TensorDataset(text_sequences)\n",
    "pred_loader = DataLoader(pred_dataset, batch_size=best_batch, shuffle=False)\n",
    "\n",
    "# 存储预测结果和概率\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in pred_loader:\n",
    "        batch_X = batch[0].to(device)\n",
    "        outputs = new_model(batch_X)\n",
    "        probs = torch.softmax(outputs, dim=1)  # 转换为概率\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        \n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "# 将结果添加回原始数据\n",
    "test_data['prediction'] = all_preds\n",
    "test_data['probabilities'] = list(np.array(all_probs))  # 存储为概率数组列\n",
    "\n",
    "# 可选：展开概率到独立列\n",
    "prob_df = pd.DataFrame(all_probs, columns=[f'prob_class_{i}' for i in range(len(all_probs[0]))])\n",
    "test_data = pd.concat([test_data, prob_df], axis=1)\n",
    "\n",
    "# 查看结果\n",
    "print(test_data[['full_text', 'prediction', 'probabilities']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dad435",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('CNN_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6397943",
   "metadata": {},
   "source": [
    "# Model 3: BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778be81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
