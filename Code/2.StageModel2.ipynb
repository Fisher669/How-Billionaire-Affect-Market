{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61062db5",
   "metadata": {},
   "source": [
    "# 机器学习因果推断——Stage2——模型2\n",
    "\n",
    "多项机器学习算法对比，利用shap方法获得解释性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6ffa7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a16c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cb5e1f",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = r\"C:\\Software\\Local Things (Coding)\\comp\\2025大学生建模比赛\\代码\\比赛项目代码\\CNN_prediction.csv\"\n",
    "stock_vol_path = r\"C:\\Software\\Local Things (Coding)\\comp\\2025大学生建模比赛\\数据\\stock_volatility.csv\"\n",
    "index_list = ['DJI', 'IXIC', 'NDX', 'NYA', 'NYID', 'OEX', 'SPX']\n",
    "pred_data = pd.read_csv(pred_path)\n",
    "stock_vol_data = pd.read_csv(stock_vol_path)\n",
    "stock_vol_data.rename(columns={\"交易日期\": \"time\"}, inplace=True)\n",
    "\n",
    "print(f\"prediction data shape: {pred_data.shape} \\n prediction data columns: {pred_data.columns}\")\n",
    "print(f\"stock volatility data shape: {stock_vol_data.shape} \\n stock volatility data columns: {stock_vol_data.columns}\")\n",
    "print(stock_vol_data[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8241dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_vol_data.hist(bins=100, figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df602853",
   "metadata": {},
   "source": [
    "## 先对于预测数据先把预测值取均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6023a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_data[['time', 'name', 'prob_class_0', 'prob_class_1', 'prob_class_2', 'prob_class_0.1', 'prob_class_1.1', 'prob_class_2.1']]\n",
    "# netural == 1 negative == 0 positive == 2\n",
    "pred_df['prob_negative'] = 1/2 * pred_df['prob_class_0'] + 1/2 * pred_df['prob_class_0.1']\n",
    "pred_df['prob_neutral'] = 1/2 * pred_df['prob_class_1'] + 1/2 * pred_df['prob_class_1.1']\n",
    "pred_df['prob_positive'] = 1/2 * pred_df['prob_class_2'] + 1/2 * pred_df['prob_class_2.1']\n",
    "pred_df_select = pred_df[['time','name', 'prob_negative', 'prob_neutral', 'prob_positive']]\n",
    "pred_df_select.to_csv('integrated_pred.csv',index=False,encoding='utf-8-sig')\n",
    "pred_df_select.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wide => long table for each probs\n",
    "\n",
    "df = pred_df_select.copy()\n",
    "df['time'] = pd.to_datetime(df['time'],errors='coerce')\n",
    "df['time'] = df['time'].dt.date\n",
    "sentiment_label = 'prob_negative'\n",
    "# sentiment_label = 'prob_neutral'\n",
    "# sentiment_label = 'prob_positive'\n",
    "\n",
    "# [0.21195608 0.57607543 0.21196853]\n",
    "# df_final = df[[\"time\", \"name\", \"prob_positive\"]]\n",
    "if sentiment_label == 'prob_negative':\n",
    "    imputation = 0.21195600\n",
    "elif sentiment_label == 'prob_neutral':\n",
    "    imputation = 0.57607543\n",
    "elif sentiment_label == 'prob_positive':\n",
    "    imputation = 0.21196853\n",
    "\n",
    "df_final = df[[\"time\", \"name\", sentiment_label]]\n",
    "\n",
    "df_grouped = df_final.groupby(['time', 'name'], as_index=False).agg({sentiment_label: 'mean'})\n",
    "\n",
    "negative = df_grouped.pivot_table(index='time', columns='name', values=sentiment_label, aggfunc='mean')\n",
    "negative = negative.reset_index()\n",
    "\n",
    "negative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stock_vol_data['time'][:3])\n",
    "stock_vol_data['time'] = pd.to_datetime(stock_vol_data['time']).dt.date\n",
    "print(stock_vol_data['time'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73159cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_data = pd.merge(negative,stock_vol_data, on = 'time',how='outer')\n",
    "full_data.drop(index = [0,1],inplace=True)\n",
    "full_data.dropna(subset=index_list,inplace=True)\n",
    "full_data.reset_index(drop=True,inplace=True)\n",
    "full_data.to_csv('regression_data.csv',index=False,encoding='utf-8-sig')\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1712a0",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf2554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('regression_data.csv')\n",
    "data.set_index('time', inplace=True)\n",
    "print(data.columns)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0941aa6b",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae639dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "cols = [ 'IXIC', 'NDX', 'DJI','BillGates', 'Eyalo365', 'JackMa', 'JeffBezos', 'MichaelDell',\n",
    "       'MukeshDhiAmbani', 'RayDalio', 'RicardoBSalinas', 'ShivNadarFDN',\n",
    "       'StevenACohen2', 'Steven_Ballmer', 'elonmusk', 'ericschmidt',\n",
    "       'esaverin', 'laurenepowell', 'leijun', 'lemannoficial',\n",
    "       'mackenziescott', 'masason', 'mcannonbrookes', 'scottfarkas',\n",
    "       'udaykotak',]\n",
    "data[cols].hist(bins=50, figsize=(15,15))\n",
    "plt.savefig('hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "corr = data[cols].corr()\n",
    "sns.heatmap(corr, annot=False, cmap='coolwarm',linewidths=0,annot_kws={\"size\": 10})\n",
    "plt.savefig('corr.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e82d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cols].describe().T.to_csv('data_describe.csv',float_format='%.3f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ec486c",
   "metadata": {},
   "source": [
    "# ML model for stage 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22f96b",
   "metadata": {},
   "source": [
    "We plan to use the following models:\n",
    "\n",
    "- Lasso\n",
    "- Ridge\n",
    "- Elastic Net\n",
    "- Random Forest\n",
    "- Bagging\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806adf7b",
   "metadata": {},
   "source": [
    "思路首先是对于单一指数进行回归，IXIC为基准\n",
    "\n",
    "然后对于其他指数回归，作为稳健型检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf86453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge,Lasso,ElasticNet,LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "\n",
    "# [0.21195608 0.57607543 0.21196853]\n",
    "\n",
    "data.fillna(imputation,inplace=True)\n",
    "threads = multiprocessing.cpu_count()-2\n",
    "X = data.drop(index_list, axis=1)\n",
    "# y = data['IXIC']\n",
    "y = data['NDX']\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(X.columns)\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af18788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, median_absolute_error, r2_score,mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "param_grids={\n",
    "    'Ridge':{\n",
    "        'ridge__alpha':[ 0.01, 0.1, 1, 10, 100,]\n",
    "    },\n",
    "    'Lasso':{\n",
    "        'lasso__alpha':[ 0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    'ElasticNet':{\n",
    "        'elasticnet__alpha':[ 0.01, 0.1, 1, 10, 100],\n",
    "        'elasticnet__l1_ratio':[0.1,0.3,0.5,0.7,0.9]\n",
    "    },\n",
    "    'SVR':{\n",
    "        'svr__C': [0.01,0.1, 1, 10, 100],\n",
    "        'svr__kernel': ['rbf'],\n",
    "        'svr__gamma': [0.01,0.1, 1, 10, 100]\n",
    "    },\n",
    "    'Decision_Tree':{\n",
    "        'decision_tree__max_depth': [2, 4, 6, 8, 10, 12, 14],\n",
    "        'decision_tree__min_samples_split': [2, 5, 10, 20]\n",
    "    },\n",
    "    'Random_Forest':{\n",
    "        'random_forest__max_depth': [2, 4, 6, 8, 10, 12, 14],\n",
    "        'random_forest__min_samples_split': [2, 5, 10, 20],\n",
    "        'random_forest__n_estimators': [10, 50, 100, 200]\n",
    "    },\n",
    "    'xgb':{\n",
    "        'xgb__max_depth': [2, 4, 6, 8, 10, 12, 14],\n",
    "        'xgb__min_child_weight': [1, 2, 3, 4, 5],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "        'xgb__n_estimators': [10, 50, 100, 200]\n",
    "    }\n",
    "}\n",
    "\n",
    "models={\n",
    "    'OLS': LinearRegression(),\n",
    "    'Ridge': Ridge(random_state=666),\n",
    "    'Lasso': Lasso(random_state=666),\n",
    "    'ElasticNet': ElasticNet(random_state=666),\n",
    "    'SVR':SVR(),\n",
    "    'Decision_Tree': DecisionTreeRegressor(),\n",
    "    'Random_Forest': RandomForestRegressor(),\n",
    "    'xgb': xgb.XGBRegressor()\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = pd.DataFrame(columns=['Model', 'Best Params', 'RMSE', 'MAE', 'MeAE', 'R-squared'])\n",
    "coef_df = pd.DataFrame()\n",
    "\n",
    "# 将数据集划分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# 其他代码保持不变\n",
    "\n",
    "# 在循环中，修改模型名称与步骤名称的一致性\n",
    "for name, model in models.items():\n",
    "    if name in param_grids:\n",
    "        # 创建管道\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            (name.lower(), model)\n",
    "        ])\n",
    "        # 设置网格搜索\n",
    "        grid = GridSearchCV(pipeline, param_grid=param_grids[name], cv=5, n_jobs=-1, scoring='neg_root_mean_squared_error', refit=True)\n",
    "        grid.fit(X_train, y_train)\n",
    "        best_model = grid.best_estimator_\n",
    "        best_params = grid.best_params_\n",
    "    else:\n",
    "        # 创建管道\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            (name.lower(), model)\n",
    "        ])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        best_model = pipeline\n",
    "        best_params = 'N/A'\n",
    "\n",
    "    # 在验证集上进行预测\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # 计算性能指标\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    medae = median_absolute_error(y_test, y_pred)\n",
    "    # r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "    # 将结果添加到 DataFrame\n",
    "    new_row = pd.DataFrame({\n",
    "        'Model': [name],\n",
    "        'Best Params': [best_params],\n",
    "        'RMSE': [rmse],\n",
    "        'MAE': [mae],\n",
    "        'MeAE': [medae],\n",
    "        # 'R-squared': [r2],\n",
    "        'MAPE': [mape]\n",
    "    })\n",
    "    results = pd.concat([results, new_row], ignore_index=True)\n",
    "\n",
    "    # 如果模型具有系数属性，保存系数\n",
    "    if hasattr(best_model.named_steps[name.lower()], 'coef_'):\n",
    "        coef = best_model.named_steps[name.lower()].coef_\n",
    "        coef_df[name] = coef\n",
    "\n",
    "# 设置系数 DataFrame 的索引\n",
    "coef_df.index = X.columns\n",
    "coef_df.index.name = 'Variables'\n",
    "\n",
    "# 输出结果\n",
    "print(results)\n",
    "print(coef_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f2b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "coef_df_tr = coef_df.transpose()\n",
    "os.makedirs('./output', exist_ok=True)\n",
    "coef_df_tr.to_csv(f'./output/{sentiment_label}_coef_df_tr.csv')\n",
    "results.to_csv(f'./output/{sentiment_label}_results.csv')\n",
    "print(f'Coef DataFrame and Results saved to output/{sentiment_label}_*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f006bca2",
   "metadata": {},
   "source": [
    "# 利用最佳模型进行分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# SHAP 与 PDP 相关包\n",
    "import shap\n",
    "from pdpbox import pdp\n",
    "\n",
    "# 假设 X, y 已经定义\n",
    "# 如果 X 是 numpy 数组，可以考虑转换为 DataFrame，例如：\n",
    "# X = pd.DataFrame(X, columns=[\"feat1\", \"feat2\", ...])\n",
    "\n",
    "# 分割数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 定义并训练决策树模型\n",
    "best_dt = DecisionTreeRegressor(max_depth=4, min_samples_split=20)\n",
    "best_dt.fit(X_train, y_train)\n",
    "y_pred = best_dt.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========= SHAP 分析 =========\n",
    "# 使用 TreeExplainer 来解释决策树模型\n",
    "explainer = shap.TreeExplainer(best_dt)\n",
    "# 计算 SHAP 值，这里对测试集数据进行解释\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "df = pd.concat([pd.DataFrame(X_test), pd.DataFrame(y_test)], axis=1)\n",
    "shap.initjs()\n",
    "\n",
    "shap_values = explainer.shap_values(df)\n",
    "shap.summary_plot(shap_values, df, plot_type='bar', show=False)\n",
    "\n",
    "# 绘制 SHAP 特征重要性图\n",
    "\n",
    "shap_sum = np.abs(shap_values).mean(axis=0)\n",
    "importance_df = pd.DataFrame([data.columns.tolist(), shap_sum.tolist()]).T\n",
    "importance_df.columns = ['column_name', 'shap_importance']\n",
    "importance_df = importance_df.sort_values('shap_importance', ascending=False)\n",
    "importance_df.to_csv('dt_SHAP_feature_importance.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b9329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "data = pd.read_csv('c:\\\\Software\\\\Local Things (Coding)\\\\comp\\\\2025大学生建模比赛\\\\代码\\\\比赛项目代码\\\\dt_SHAP_feature_importance.csv')\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体字体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "# 设置图形大小\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 绘制条形图\n",
    "plt.bar(data['column_name'], data['shap_importance'], color='skyblue')\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('各个富豪对于股市波动率的SHAP特征重要性')\n",
    "plt.xlabel('富豪推文情感标签')\n",
    "plt.ylabel('SHAP特征重要性')\n",
    "\n",
    "# 旋转x轴标签以便更好地显示\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.savefig('dt_SHAP_feature_importance.png', dpi=300)\n",
    "# 显示图形\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
